{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOjxFB4e8OyOSgchTrHUnA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spyysalo/dl-binf-summer-school-2025/blob/main/unpacking_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unpacking `pipeline`\n",
        "\n",
        "This notebook illustrates some parts that go into text generation using the `pipeline` class."
      ],
      "metadata": {
        "id": "upiVhm3M7i7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First install the [transformers](https://huggingface.co/docs/transformers/index) package."
      ],
      "metadata": {
        "id": "-ImKmG7j70Z1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8jS-wINx56E5"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load a `pipeline` for text generation with a small model."
      ],
      "metadata": {
        "id": "jIPvUiXkoK3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "MODEL_NAME = 'HuggingFaceTB/SmolLM-135M'\n",
        "\n",
        "pipe = pipeline(\n",
        "    'text-generation',\n",
        "    model=MODEL_NAME,\n",
        "    device_map='auto',\n",
        ")"
      ],
      "metadata": {
        "id": "8Haw6sWQo9s8",
        "outputId": "20312272-22a9-4c8e-eb82-259871a30054",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conveniently generate text using the high-level abstraction that `pipeline` provides:"
      ],
      "metadata": {
        "id": "fuRhqTzLuTMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The capital of Denmark is'\n",
        "\n",
        "print(pipe(prompt)[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyvybclMufGL",
        "outputId": "a8aae17a-2343-44ee-822c-35e60c226413"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Denmark is Copenhagen, with a population of 534,966 people. The county is roughly equivalent to Vermont, but contains a total area of 3,489.07 square miles, making it the third-largest state in the United States. It is one of the oldest states with a history going back to 1624 when the Swedish colony of New Sweden was established. Today it is a member of the Commonwealth of Nations along with New Zealand and Australia.\n",
            "Denmark was originally inhabited by the Sami people who lived in what is now Scandinavia for thousands of years. The Viking settlers brought with them their own distinct culture, language, and beliefs. The country was later invaded by the Norse, who established a kingdom in Greenland in the 9th century. When Denmark was conquered by Denmark, Sweden, Norway, and Iceland, they were all under Danish rule.\n",
            "Denmark has a long history of trade with other countries, including the United States. The country was a member of the European Economic Community (EEC). In 1958, the British government recognized Denmark as an independent nation and granted them diplomatic recognition. In 1961, Denmark became a member of the European Economic Community (EEC). In \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity, let's look at generating one word using greedy decoding, i.e. simply selecting the word that's most likely according to the model."
      ],
      "metadata": {
        "id": "sC7CXiDFvlpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    'do_sample': False,\n",
        "    'max_new_tokens': 1,\n",
        "}\n",
        "\n",
        "print(pipe(prompt, **params)[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLCvXb0Vv1GW",
        "outputId": "7e0a6559-9141-4682-e1a3-0719b9b09007"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Denmark is Copenhagen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at what's going on behind the `pipeline` abstraction. First, here's the model"
      ],
      "metadata": {
        "id": "795RV2T0wLnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipe.model\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d41xTKy7wa-q",
        "outputId": "2484427f-17dc-4d6f-b4d5-cebfcae3a35a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(49152, 576)\n",
            "    (layers): ModuleList(\n",
            "      (0-29): 30 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model doesn't actually deal with text directly, but rather with token indices. The mapping between running text and token indices is implemented by a tokenizer."
      ],
      "metadata": {
        "id": "BliQAAUiwrhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = pipe.tokenizer"
      ],
      "metadata": {
        "id": "4F6Ss_LUwclI"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the mapping between our prompt and the token indices."
      ],
      "metadata": {
        "id": "OUgQ0FZZxBKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ = tokenizer(prompt)\n",
        "\n",
        "print(input_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajSrSCJRw5xK",
        "outputId": "2b5d829b-4e79-4212-d88f-fda30c3a71c2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [504, 3575, 282, 15644, 314], 'attention_mask': [1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's the actual input to the model, and `input_ids` are the token indices. (We can ignore the `attention_mask` here.)\n",
        "\n",
        "The tokenizer can map these back to text"
      ],
      "metadata": {
        "id": "pcLaMwi_xN9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens(input_.input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W8aczlQxqqX",
        "outputId": "5cd56fed-7597-4b10-d15c-614148aebe57"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Ġcapital', 'Ġof', 'ĠDenmark', 'Ġis']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(The `Ġ` there encodes space; this representation is a minor quirk of the GPT tokenizer.)\n",
        "\n",
        "Because the token ids represent both visible characters and space, the full input string can be reconstructed accurately:"
      ],
      "metadata": {
        "id": "MWu8qcduyEIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(input_.input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J47fG7k6yA9c",
        "outputId": "634bb881-8cdd-4722-a413-fa2baaa2941f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Denmark is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can invoke the model directly with the encoded input. Here we need to ask the tokenizer to generate pytorch tensors due to some implementation details, but the information content is the same."
      ],
      "metadata": {
        "id": "IDM-b-jwyraj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "print(input_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUFYYdcYzrYo",
        "outputId": "9e174bfa-a70e-4e73-da7e-2e86c7560d7b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  504,  3575,   282, 15644,   314]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**input_)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXRPVpQWyngG",
        "outputId": "7db6f5f6-5f38-418f-a352-cd54f511666c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CausalLMOutputWithPast(loss=None, logits=tensor([[[ -3.5625, -11.6875, -11.6875,  ...,  -7.3750,  -7.0312,  -9.8750],\n",
            "         [  2.0938,  -8.3125,  -8.4375,  ...,  -7.7812,  -5.3125,  -9.5000],\n",
            "         [-10.4375, -20.6250, -20.7500,  ..., -16.1250, -14.0625, -19.2500],\n",
            "         [ 12.8750,  -4.2188,  -4.5000,  ...,   1.7891,   1.9297,  -2.0938],\n",
            "         [ -0.0728, -13.3125, -13.5625,  ...,  -8.6250,  -3.8594, -12.6875]]],\n",
            "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>), past_key_values=DynamicCache(layers=[<transformers.cache_utils.DynamicLayer object at 0x78d8f938dd50>, <transformers.cache_utils.DynamicLayer object at 0x78d8f9380e90>, <transformers.cache_utils.DynamicLayer object at 0x78d8f9382cd0>, <transformers.cache_utils.DynamicLayer object at 0x78d8f9380b50>, <transformers.cache_utils.DynamicLayer object at 0x78d8f9380c90>, <transformers.cache_utils.DynamicLayer object at 0x78d8f9380b90>, <transformers.cache_utils.DynamicLayer object at 0x78d8f93800d0>, <transformers.cache_utils.DynamicLayer object at 0x78d8f9380d90>, <transformers.cache_utils.DynamicLayer object at 0x78d8f9382c10>, <transformers.cache_utils.DynamicLayer object at 0x78d8f93835d0>, <transformers.cache_utils.DynamicLayer object at 0x78d8f9383ed0>, <transformers.cache_utils.DynamicLayer object at 0x78d86b5d1510>, <transformers.cache_utils.DynamicLayer object at 0x78d8f9381750>, <transformers.cache_utils.DynamicLayer object at 0x78d86bbd6250>, <transformers.cache_utils.DynamicLayer object at 0x78d86ba416d0>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58ff10>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58fcd0>, <transformers.cache_utils.DynamicLayer object at 0x78d86b64f450>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58f390>, <transformers.cache_utils.DynamicLayer object at 0x78d8f9380690>, <transformers.cache_utils.DynamicLayer object at 0x78d8f938f950>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58f990>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58fc90>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58f1d0>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58f9d0>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58f690>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58f510>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58fa10>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58e150>, <transformers.cache_utils.DynamicLayer object at 0x78d86b58f750>]), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary output of the model are the logits, which correspond to unnormalized scores for each token. We're interested in the scores for the last token. (The first dimension here is for the batch, and we have a batch of one.)"
      ],
      "metadata": {
        "id": "9qRlDUDY0DOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.logits.shape)\n",
        "\n",
        "logits = output.logits[0][-1]\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D92PA1Q7y50u",
        "outputId": "27755c56-fb20-453b-fcb1-25f238420b2a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 49152])\n",
            "tensor([ -0.0728, -13.3125, -13.5625,  ...,  -8.6250,  -3.8594, -12.6875],\n",
            "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For greedy decoding, we can just take the argmax, which gives us the index of the most likely next word."
      ],
      "metadata": {
        "id": "0Y0IJMkO1pPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits.argmax()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHHoDolY0Szv",
        "outputId": "0132d5c6-5f3a-47fe-cea7-fb902e7abb22"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(27101)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens([logits.argmax()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8THCuvx0Xnr",
        "outputId": "6e71874e-d970-4800-cf3a-0418137a76ad"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ĠCopenhagen']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we wanted to continue generating more than one word, we would simply append this index to `input_ids` and invoke the model again."
      ],
      "metadata": {
        "id": "70_YUeW615VU"
      }
    }
  ]
}